{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import copy\n",
    "\n",
    "data = pd.read_csv('./Dataset/data.csv')\n",
    "\n",
    "# Here's how you reduce sample the data set. sample_data will contain every 100th row. This is just as an example\n",
    "# Should use more than a 100th of the data\n",
    "sample_data = data.iloc[::2000, :]\n",
    "sample_data.head()\n",
    "\n",
    "# Splitting the data into features and song profiles\n",
    "song_profiles = sample_data[['id', 'name', 'artists', 'release_date', 'year']].copy()\n",
    "features = sample_data.copy().drop('name', axis=1).drop('artists', axis=1).drop('id', axis=1).drop('release_date', axis=1).values.tolist()\n",
    "def Maxes(features):\n",
    "    maxes = [0 for _ in range(len(features[0]))]\n",
    "    for row in features:\n",
    "        for i in range(len(row)):\n",
    "            if (abs(row[i]) > maxes[i]):\n",
    "                maxes[i] = abs(row[i])\n",
    "    return maxes\n",
    "\n",
    "def CalcNormalizations(features, maxes):\n",
    "    ofeatures = copy.deepcopy(features)\n",
    "    length = len(ofeatures)\n",
    "    for row in ofeatures:\n",
    "        for i in range(len(row)):\n",
    "            row[i] = (row[i])/maxes[i]\n",
    "    return ofeatures\n",
    "\n",
    "def SimilarityCalculation(input_class, output_class):\n",
    "    numer = 0.0\n",
    "    mag1 = 0.0\n",
    "    mag2 = 0.0\n",
    "\n",
    "    for i in range(len(input_class)):\n",
    "        numer += (input_class[i] * output_class[i])\n",
    "        mag1 += input_class[i] ** 2\n",
    "        mag2 += output_class[i] ** 2\n",
    "    return numer / ((mag1 * mag2) ** 0.5)\n",
    "\n",
    "def GetClassFromNum(num):\n",
    "    out = [0 for _ in range(20)] #we have 600k songs as our input, thus we need 20 bits to hold any as a \n",
    "    i = 0\n",
    "    while(num > 0):\n",
    "        out[i] = num % 2\n",
    "        num = num // 2\n",
    "        i += 1\n",
    "    return out\n",
    "\n",
    "def GetNumFromClass(vals):\n",
    "    out = 0\n",
    "    for i in range(len(vals)):\n",
    "        out += (2 ** i) * vals[i]\n",
    "    return out\n",
    "\n",
    "def CalculateClasses(data):\n",
    "    # just takes the number in the list, and returns it as a little-endian\n",
    "    # binary representation\n",
    "    output = []\n",
    "    for i in range(len(data)):\n",
    "        data[i].append(GetClassFromNum(i))\n",
    "    return data\n",
    "\n",
    "maxes = Maxes(features)\n",
    "normalized_features = CalcNormalizations(features, maxes)\n",
    "normalized_data_with_classes = CalculateClasses(normalized_features)\n",
    "# Drop irrelevant columns in feature set\n",
    "# features = features.drop('key', axis=1)\n",
    "    # Key?\n",
    "    # My guess is that the key is the western music scale key that the song is in\n",
    "    # possibly normalized for major scales? (e.g., gmaj == cmin)\n",
    "# sample_data\n",
    "# song_profiles.head()\n",
    "# features.head()\n",
    "# I'm going to make this an RNN. Deal.\n",
    "# we can use the index of the song in the features list as the class. Because we're "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import copy\n",
    "# We define a our recurrent neural network as one where the features of the current song being played is the input, along \n",
    "# along with classifiers (i.e., their place in the features array) as other inputs.\n",
    "# we return the value as a binary number representation of the index\n",
    "class Neuron:\n",
    "    Weights = []\n",
    "    Output = 1.0\n",
    "    Delta = 0.5\n",
    "    \n",
    "    def __init__(self, prevLayerWidth):\n",
    "        self.Weights = [random.random() for _ in range(prevLayerWidth + 1)]\n",
    "    \n",
    "    def Activate(self, inputs):\n",
    "        activation_val = self.Weights[-1]\n",
    "#         print(\"\\tsanity check:\\n\\t\\t\", inputs, len(inputs), \"\\n\\t\\t\", self.Weights, len(self.Weights))\n",
    "        for i in range(len(inputs)):\n",
    "            activation_val += inputs[i] * self.Weights[i]\n",
    "        self.Output = max(0, activation_val)\n",
    "        #print(\"output sanity check:\", self.Output)\n",
    "        return self.Output\n",
    "    \n",
    "    def UpdateWeights(self, regressive_outputs, learn_rate):\n",
    "#         print(\"UpdateWeights sanity check: \", regressive_outputs)\n",
    "        for i in range(len(regressive_outputs)):\n",
    "            self.Weights[i] += learn_rate * self.Delta * regressive_outputs[i]\n",
    "        self.Weights[-1] += learn_rate * self.Delta\n",
    "    \n",
    "    def UpdateDelta(self, error):\n",
    "        self.Delta = error * (0 if self.Output < 1 else 1)\n",
    "\n",
    "class RecurrentNeuralNetwork:\n",
    "    RecurrentInputs = 0\n",
    "    OutputLength = 0\n",
    "    NeuronLayers = []\n",
    "    LearningRate = 0.0\n",
    "    \n",
    "    def __init__(self, features, recurrent_inputs, hidden_layers, output_length, learning_rate):\n",
    "        print(\"Building neural network with\", features, \"features,\", recurrent_inputs, \"recurrent inputs, and\", hidden_layers, \"hidden layers.\")\n",
    "        self.RecurrentInputs = recurrent_inputs\n",
    "        self.LearningRate = learning_rate\n",
    "        self.OutputLength = output_length\n",
    "        \n",
    "        y_intercept = features + recurrent_inputs * output_length\n",
    "        slope = (output_length - y_intercept)/hidden_layers #for quicker learning, we narrow the scope of each layer a bit\n",
    "        prev_width = y_intercept\n",
    "        for i in range(hidden_layers + 1):\n",
    "            next_len = math.ceil((i*slope + y_intercept)) if (i < hidden_layers) else output_length\n",
    "            print(\"\\tBuilding layer\", i, \"with\", next_len, \"neurons.\")\n",
    "            for j in range(next_len):\n",
    "                self.NeuronLayers.append([Neuron(prev_width) for _ in range(next_len)])\n",
    "            prev_width = next_len\n",
    "        print(\"Completed!\")\n",
    "    \n",
    "    def ForwardPropagation(self, current_input, recurrent_inputs):\n",
    "        # setup previous layers \"output\" as the total input\n",
    "        layer_output = copy.deepcopy(current_input)\n",
    "#         print(\"\\tsanity check: \", recurrent_inputs)\n",
    "        for recurrent_input in recurrent_inputs:\n",
    "            for el in recurrent_input:\n",
    "                layer_output.append(el)\n",
    "        \n",
    "        for layer in self.NeuronLayers:\n",
    "            next_layer_input = []\n",
    "            for neuron in layer:\n",
    "                next_layer_input.append(neuron.Activate(layer_output))\n",
    "            layer_output = next_layer_input\n",
    "        return layer_output\n",
    "    \n",
    "    def BackwardPropagation(self, expected_value):\n",
    "        prev_layer = []\n",
    "        for layer in reversed(self.NeuronLayers):\n",
    "            layer_error = []\n",
    "#             print(\"\\t\\t\", len(expected_value), \",\", len(layer))\n",
    "            if (layer is self.NeuronLayers[-1]):\n",
    "                for i in range(len(layer)):\n",
    "                    layer_error.append(expected_value[i] - layer[i].Output)\n",
    "            else:\n",
    "                for i in range(len(layer)):\n",
    "                    error = 0.0\n",
    "                    for neuron in prev_layer:\n",
    "                        error += neuron.Weights[i] * neuron.Delta\n",
    "                    layer_error.append(error)\n",
    "            for i in range(len(layer)):\n",
    "                layer[i].UpdateDelta(layer_error[i])\n",
    "            prev_layer = layer\n",
    "\n",
    "    def UpdateAllWeights(self, current_input, recurrent_inputs):\n",
    "        current_input = copy.deepcopy(current_input)\n",
    "        layer_input = current_input\n",
    "        for recurrent_input in recurrent_inputs:\n",
    "            for el in recurrent_input:\n",
    "                layer_input.append(el)\n",
    "#         print(\"updateallweights sanity:\", layer_input)\n",
    "        for layer in self.NeuronLayers:\n",
    "            next_layer_input = []\n",
    "            for neuron in layer:\n",
    "                neuron.UpdateWeights(layer_input, self.LearningRate)\n",
    "                next_layer_input.append(neuron.Output)\n",
    "            layer_input = next_layer_input\n",
    "\n",
    "    def SimilarityCalculation(self, vector01, vector02):\n",
    "        numer = 0.0\n",
    "        mag1 = 0.0\n",
    "        mag2 = 0.0\n",
    "\n",
    "        for i in range(len(vector01)):\n",
    "            numer += (vector01[i] * vector02[i])\n",
    "            mag1 += vector01[i] ** 2\n",
    "            mag2 += vector02[i] ** 2\n",
    "        return numer / ((mag1 * mag2) ** 0.5)\n",
    "\n",
    "    def FindClosestInTrainingData(self, output_class, training_data):\n",
    "        output = training_data[0]\n",
    "        max_similarity = 0\n",
    "        for t in training_data:\n",
    "            current_similarity = self.SimilarityCalculation(output_class, t)\n",
    "            if (current_similarity > max_similarity):\n",
    "                max_similarity = current_similarity\n",
    "                output = t\n",
    "        return output\n",
    "\n",
    "    def Train(self, training_set, epochs):\n",
    "        shuffled_set = copy.copy(training_set)\n",
    "        print(\"RNN starting training with\", epochs, \"epochs and\",\n",
    "            self.RecurrentInputs, \"recurrent inputs on\",\n",
    "            len(training_set),\"pieces of data.\")\n",
    "        prev_inputs = []\n",
    "        expected_similarity = 0.85\n",
    "        blank = [0 for _ in range(self.OutputLength)]\n",
    "\n",
    "        for i in range(self.RecurrentInputs):\n",
    "            prev_inputs.append(copy.deepcopy(blank))\n",
    "        \n",
    "        for epoch in range(epochs + 1):\n",
    "            print(\"Epoch:\", epoch + 1)\n",
    "            total_epoch_error = 0.0\n",
    "            random.shuffle(shuffled_set)\n",
    "            \n",
    "            j = 1\n",
    "            for value in shuffled_set:\n",
    "#                 print(\"prepossible sanity\", prev_inputs)\n",
    "                possible_output = self.ForwardPropagation(value[:-1], prev_inputs)[:-1] # drop biaser for actual class\n",
    "                print(\"posout:\", possible_output)\n",
    "                calculated_class = [0 if el <= 0 else 1 for el in possible_output]\n",
    "                index = GetNumFromClass(calculated_class)\n",
    "\n",
    "#                 print(\"sanity:\", calculated_class)\n",
    "                if (index < len(training_set)):\n",
    "                    calculated_song = training_set[index]\n",
    "                    similarity = self.SimilarityCalculation(value[:-1], calculated_song[:-1])\n",
    "                    print(\"\\t\", j,\"Index:\", index, \"\\tSimilarity: \", similarity)\n",
    "                else:\n",
    "                    calculated_class = copy.deepcopy(value[-1])\n",
    "                    similarity = 0\n",
    "                    print(\"\\t\", j, \"Index:\", index, \"\\tOOB, 0 similarity\")\n",
    "                next_recurrent_inputs = prev_inputs\n",
    "\n",
    "                if (similarity < expected_similarity or similarity == 1.0):\n",
    "                    calculated_class.append(0)\n",
    "                else:\n",
    "                    calculated_class.append(1)\n",
    "                    next_recurrent_inputs = prev_inputs[1:]\n",
    "                    next_recurrent_inputs.append(calculated_class)\n",
    "\n",
    "#                 print(\"\\texpected:\", expected)\n",
    "                total_epoch_error = (1 - calculated_class[-1]) ** 2 \n",
    "#                 print(\"sanity:\", calculated_class)\n",
    "                self.BackwardPropagation(calculated_class)\n",
    "                self.UpdateAllWeights(value[:-1], prev_inputs)\n",
    "                prev_inputs = next_recurrent_inputs\n",
    "                \n",
    "                j += 1\n",
    "            print(\"\\n\\n\\tError:\", total_epoch_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building neural network with 15 features, 2 recurrent inputs, and 3 hidden layers.\n",
      "\tBuilding layer 0 with 57 neurons.\n",
      "\tBuilding layer 1 with 45 neurons.\n",
      "\tBuilding layer 2 with 33 neurons.\n",
      "\tBuilding layer 3 with 21 neurons.\n",
      "Completed!\n",
      "RNN starting training with 20 epochs and 2 recurrent inputs on 88 pieces of data.\n",
      "Epoch: 1\n",
      "posout: [1.2972372668780255e+204, 1.4085224174061907e+204, 1.2208593491798322e+204, 1.339210207262176e+204, 9.575061705040711e+203, 1.1282936089726316e+204, 1.2125852007753639e+204, 1.3414716934471215e+204, 1.2738207887441367e+204, 1.2388711524195376e+204, 1.3673879216645747e+204, 1.3526554016999603e+204, 1.293204152555941e+204, 1.2223916125079887e+204, 1.2916522975531377e+204, 9.809195841395378e+203, 1.3081856517569434e+204, 1.3467481918956527e+204, 1.2210277257559048e+204, 9.462196955151948e+203]\n",
      "\t 1 Index: 1048575 \tOOB, 0 similarity\n",
      "posout: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\t 2 Index: 0 \tSimilarity:  0.6387390740209391\n",
      "posout: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\t 3 Index: 0 \tSimilarity:  0.8341495352618961\n",
      "posout: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\t 4 Index: 0 \tSimilarity:  0.8168531780866276\n",
      "posout: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\t 5 Index: 0 \tSimilarity:  0.6557183102015529\n",
      "posout: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\t 6 Index: 0 \tSimilarity:  0.7509212481047899\n",
      "posout: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\t 7 Index: 0 \tSimilarity:  0.8399263206734526\n",
      "posout: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\t 8 Index: 0 \tSimilarity:  0.7152260667545226\n",
      "posout: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\t 9 Index: 0 \tSimilarity:  0.7317036402007472\n",
      "posout: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\t 10 Index: 0 \tSimilarity:  0.6111130871333893\n",
      "posout: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\t 11 Index: 0 \tSimilarity:  0.6605874264228607\n",
      "posout: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\t 12 Index: 0 \tSimilarity:  0.8504230523814847\n",
      "posout: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\t 13 Index: 0 \tSimilarity:  0.6704686718053277\n",
      "posout: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\t 14 Index: 0 \tSimilarity:  0.758635981954525\n",
      "posout: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\t 15 Index: 0 \tSimilarity:  0.7251033792125338\n",
      "posout: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\t 16 Index: 0 \tSimilarity:  0.841455308420603\n",
      "posout: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\t 17 Index: 0 \tSimilarity:  0.5663830177932992\n",
      "posout: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\t 18 Index: 0 \tSimilarity:  0.6879682677966216\n",
      "posout: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\t 19 Index: 0 \tSimilarity:  0.7911911129858482\n",
      "posout: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\t 20 Index: 0 \tSimilarity:  0.6895048443114091\n",
      "posout: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\t 21 Index: 0 \tSimilarity:  0.8431887623876727\n",
      "posout: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\t 22 Index: 0 \tSimilarity:  0.6440311549897213\n",
      "posout: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\t 23 Index: 0 \tSimilarity:  0.7303977921798966\n",
      "posout: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\t 24 Index: 0 \tSimilarity:  0.7711544740153593\n",
      "posout: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\t 25 Index: 0 \tSimilarity:  0.7358104165336812\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-148-cc248115d5db>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtest_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnormalized_data_with_classes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mrnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRecurrentNeuralNetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_features\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_features\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-146-aaf8111bb36e>\u001b[0m in \u001b[0;36mTrain\u001b[1;34m(self, training_set, epochs)\u001b[0m\n\u001b[0;32m    168\u001b[0m                 \u001b[0mtotal_epoch_error\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mcalculated_class\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m**\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[1;31m#                 print(\"sanity:\", calculated_class)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBackwardPropagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcalculated_class\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUpdateAllWeights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprev_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m                 \u001b[0mprev_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_recurrent_inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-146-aaf8111bb36e>\u001b[0m in \u001b[0;36mBackwardPropagation\u001b[1;34m(self, expected_value)\u001b[0m\n\u001b[0;32m     81\u001b[0m                     \u001b[0merror\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0mneuron\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mprev_layer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m                         \u001b[0merror\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mneuron\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWeights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mneuron\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDelta\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m                     \u001b[0mlayer_error\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_features = normalized_data_with_classes\n",
    "rnn = RecurrentNeuralNetwork(len(test_features[0]) - 1, 2, 3, len(test_features[0][-1]) + 1, 0.5)\n",
    "rnn.Train(test_features, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# this doesn't work, and I don't know why.\n",
    "\n",
    "import copy\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()\n",
    "\n",
    "class TensorFlowColaborativeNetwork:\n",
    "    Session = None\n",
    "    Graph = None\n",
    "    RecurrentInputs = 0\n",
    "    OutputLength = 0\n",
    "    Layers = []\n",
    "    LearningRate = 0.0\n",
    "    TrainingData = []\n",
    "\n",
    "    def __init__(self, training_data, recurrent_inputs, hidden_layers, output_length, learning_rate):\n",
    "        print(\"Building neural network with\", len(training_data[0]), \"features,\", recurrent_inputs, \"recurrent inputs, and\", hidden_layers, \"hidden layers.\")\n",
    "        self.TrainingData = training_data\n",
    "        self.Graph = tf.Graph().as_default()\n",
    "        self.RecurrentInputs = recurrent_inputs\n",
    "        self.LearningRate = learning_rate\n",
    "        self.OutputLength = output_length\n",
    "        \n",
    "        features = len(training_data[0])\n",
    "        y_intercept = features * (1 + recurrent_inputs)\n",
    "        slope = (output_length - y_intercept)/hidden_layers\n",
    "        prev_width = y_intercept\n",
    "        \n",
    "        song_features  = tf.placeholder(tf.int32, shape=(features))\n",
    "        song_var = tf.Variable(tf.random_normal([len(training_data), features], stddev=0.5), name=\"Song-Features\")\n",
    "        embedding = tf.keras.layers.Flatten()(tf.nn.embedding_lookup(song_features, song_var))\n",
    "        recurrency = [copy.deepcopy(embedding) for _ in range(recurrent_inputs + 1)]\n",
    "        concatenated = tf.keras.layers.concatenate(recurrency)\n",
    "        dropout = tf.keras.layers.Dropout(0.2)(concatenated)\n",
    "        \n",
    "        for i in range(hidden_layers + 1):\n",
    "            next_len = math.ceil((i*slope + y_intercept)) if (i < hidden_layers) else output_length\n",
    "            lname = \"Layer-\"+str(i)\n",
    "            bname = \"BatchNorm-\"+str(i)\n",
    "            dname = \"Dropout-\"+str(i)\n",
    "            \n",
    "            self.Layers.append(tf.keras.layers.Dense(next_len, activation = 'relu', name = lname)(dropout if len(self.Layers) == 0 else self.Layers[-1]))\n",
    "            if (len(self.Layers) < 2):\n",
    "                normalized = tf.keras.layers.BatchNormalization(name = bname)(self.Layers[0])\n",
    "                layerdropout = tf.keras.layers.Dropout(0.2, name = dname)(normalized)\n",
    "        output_layer = tf.keras.layers.Dense(1, kernel_initializer=\"lecun_uniform\", name=\"Layer-Output\")(self.Layers[-1])\n",
    "        \n",
    "        labels = tf.placeholder(tf.int32, shape=(1))\n",
    "        loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=our_labels, logits=output_layer))\n",
    "        opt = tf.train.AdamOptimizer(learning_rate = self.LearningRate)\n",
    "        init = tf.global_variable_initialier()\n",
    "        self.Session = tf.Session(config=none, graph=self.Graph)\n",
    "        self.Session.run(init)\n",
    "tflownet = TensorFlowColaborativeNetwork(features, 20, 20, len(features[0]), 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.tanh(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
