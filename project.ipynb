{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of data adjust\n",
      "Removing songs with multiple artists...\n",
      "Retrieving only 10 artists\n",
      "Normalizing the data...\n",
      "Sorting into training and testing sets...\n",
      "Form data for tensorflow\n",
      "Complete!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import copy\n",
    "import re\n",
    "\n",
    "print(\"Start of data adjust\")\n",
    "data = pd.read_csv('./Dataset/data.csv')\n",
    "row_swaps = ['acousticness',\n",
    " 'danceability',\n",
    " 'duration_ms',\n",
    " 'energy',\n",
    " 'explicit',\n",
    " 'id',\n",
    " 'instrumentalness',\n",
    " 'key',\n",
    " 'liveness',\n",
    " 'loudness',\n",
    " 'mode',\n",
    " 'name',\n",
    " 'popularity',\n",
    " 'release_date',\n",
    " 'speechiness',\n",
    " 'tempo',\n",
    " 'valence',\n",
    " 'year',\n",
    " 'artists']\n",
    "\n",
    "# Here's how you reduce sample the data set.\n",
    "# sample_data will contain every 100th row. This is just as an example\n",
    "# Should use more than a 100th of the data\n",
    "sample_data = data.iloc[::, :]\n",
    "sample_data.head()\n",
    "sample_data = sample_data[row_swaps]\n",
    "\n",
    "# Splitting the data into features and song profiles\n",
    "song_profiles = sample_data[['id', 'name', 'artists', 'release_date', 'year']].copy()\n",
    "onlyartists = song_profiles.copy().filter('artists').values\n",
    "features = (sample_data.copy()\n",
    "    .drop('name', axis=1)\n",
    "    .drop('id', axis=1)\n",
    "    .drop('release_date', axis=1)\n",
    "    .values.tolist())\n",
    "\n",
    "def RemoveMultiArtistSongs(features):\n",
    "    expr = re.compile(\"\\'\\,\")\n",
    "    output = []\n",
    "    for row in features:\n",
    "        artist_str = row[-1]\n",
    "        if expr.match(artist_str) == None:\n",
    "            output.append(row)\n",
    "    return output\n",
    "\n",
    "def GetOnly90sSongs(features):\n",
    "    outfeatures = []\n",
    "    for row in features:\n",
    "        if row[-2] >= 1990.0 and row[-2] < 2000.0:\n",
    "            outfeatures.append(row)\n",
    "    return outfeatures\n",
    "\n",
    "def GetOnly10ArtistsSongs(features):\n",
    "    artists = set()\n",
    "    out = []\n",
    "    while len(artists) < 10:\n",
    "        row = random.sample(features, 1)\n",
    "        artists.add(row[0][-1])\n",
    "    \n",
    "    for row in features:\n",
    "        if (row[-1] in artists):\n",
    "            out.append(row)\n",
    "    return out, artists\n",
    "\n",
    "# def CalcClass(value, length):\n",
    "#     output = [-1 for _ in range(length)]\n",
    "    \n",
    "#     i = 0\n",
    "#     while(value > 0):\n",
    "#         output[i] = 1 if (value % 2 == 1) else -1\n",
    "#         value = value // 2\n",
    "#         i += 1\n",
    "\n",
    "#     return output\n",
    "\n",
    "def CalcNormalizations(features):\n",
    "    maxes = [0 for _ in range(len(features[0]))]\n",
    "    mins = [30000 for _ in range (len(features[0]))]\n",
    "    for row in features:\n",
    "        for i in range(len(row) - 1):\n",
    "            if (abs(row[i]) > maxes[i]):\n",
    "                maxes[i] = abs(row[i])\n",
    "            if (row[i] < mins[i]):\n",
    "                mins[i] = row[i]\n",
    "    \n",
    "    ofeatures = copy.deepcopy(features)\n",
    "    length = len(ofeatures[0])\n",
    "    \n",
    "    for row in ofeatures:\n",
    "        for i in range(len(row) - 1):\n",
    "            row[i] = (row[i] - mins[i])/(maxes[i] - mins[i])\n",
    "    \n",
    "    for row in ofeatures:\n",
    "        for i in range(length - 1):\n",
    "            row[i] = row[i]/maxes[i]\n",
    "\n",
    "    return ofeatures\n",
    "\n",
    "def GetNumClasses(ofeatures):\n",
    "    classes = set()\n",
    "    for row in ofeatures:\n",
    "        classes.add(row[-1])\n",
    "    return len(classes)\n",
    "\n",
    "print(\"Removing songs with multiple artists...\")\n",
    "noMultipleArtists = RemoveMultiArtistSongs(features)\n",
    "# print(\"Getting all songs from the 90's...\")\n",
    "# _90ssongs = GetOnly90sSongs(features)\n",
    "print(\"Retrieving only 10 artists\")\n",
    "# _10AristsSongs, Artists = GetOnly10ArtistsSongs(_90ssongs)\n",
    "_10ArtistsSongs, Artists = GetOnly10ArtistsSongs(noMultipleArtists)\n",
    "Artists = list(Artists)\n",
    "# Artists = onlyartists\n",
    "print(\"Normalizing the data...\")\n",
    "normalized_data_with_classes = CalcNormalizations(_10ArtistsSongs)\n",
    "# num_classes = GetNumClasses(features)\n",
    "print(\"Sorting into training and testing sets...\")\n",
    "train_features = random.sample(normalized_data_with_classes, len(normalized_data_with_classes)*2//3)\n",
    "test_features = []\n",
    "for row in normalized_data_with_classes:\n",
    "    if row not in train_features:\n",
    "        test_features.append(row)\n",
    "\n",
    "print(\"Form data for tensorflow\")\n",
    "tf_train_features = []\n",
    "tf_train_labels = []\n",
    "tf_test_features = []\n",
    "tf_test_labels = []\n",
    "\n",
    "for row in train_features:\n",
    "    tf_train_features.append(row[:-1])\n",
    "    tf_train_labels.append(Artists.index(row[-1]))\n",
    "\n",
    "for row in test_features:\n",
    "    tf_test_features.append(row[:-1])\n",
    "    tf_test_labels.append(Artists.index(row[-1]))\n",
    "tf_dataset = (tf_train_features, tf_train_labels, tf_test_features, tf_test_labels)\n",
    "print(\"Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "div.output_area pre {\n",
       "    white-space: pre;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "div.output_area pre {\n",
    "    white-space: pre;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import copy\n",
    "\n",
    "debug = False\n",
    "info_dump = False\n",
    "\n",
    "class Neuron:\n",
    "    Weights = []\n",
    "    Output = 1.0\n",
    "    Delta = 0.5\n",
    "    \n",
    "    def __init__(self, layerWidth):\n",
    "        self.Weights = [random.random() for _ in range(layerWidth + 1)]\n",
    "    \n",
    "    def Activate(self, inputs):\n",
    "        activation_val = self.Weights[-1]\n",
    "        if (debug):\n",
    "            print(\"\\tsanity check:\\n\\t\\t\", inputs, len(inputs), \"\\n\\t\\t\", self.Weights, len(self.Weights))\n",
    "        for i in range(len(inputs)):\n",
    "            activation_val += inputs[i] * self.Weights[i]\n",
    "        self.Output = math.tanh(activation_val)\n",
    "        if (debug):\n",
    "            print(\"\\toutput sanity check:\", self.Output)\n",
    "        return self.Output\n",
    "    \n",
    "    def UpdateWeights(self, regressive_outputs, learn_rate):\n",
    "        if (debug):\n",
    "            print(\"UpdateWeights sanity check\\n\", regressive_outputs)\n",
    "            print(\"\\tUpdateWeights Lengths:\", len(self.Weights), len(regressive_outputs))\n",
    "        for i in range(len(regressive_outputs)):\n",
    "            self.Weights[i] += learn_rate * self.Delta * regressive_outputs[i]\n",
    "        self.Weights[-1] += learn_rate * self.Delta\n",
    "    \n",
    "    def UpdateDelta(self, error):\n",
    "        self.Delta = error * (1 - np.tanh(self.Output)**2)\n",
    "\n",
    "class DenseNeuralNetwork:\n",
    "    def __init__(self, features, hidden_layers, output_length, learning_rate):\n",
    "        self.RecurrentInputs = 0\n",
    "        self.OutputLength = 0\n",
    "        self.NeuronLayers = []\n",
    "        self.LearningRate = 0.0\n",
    "        print(\"Building neural network with\", features, \"features, and\", hidden_layers, \"hidden layers.\")\n",
    "        self.LearningRate = learning_rate\n",
    "        self.OutputLength = output_length\n",
    "        \n",
    "        y_intercept = features\n",
    "        slope = (output_length - y_intercept)/hidden_layers\n",
    "        num_weights = y_intercept\n",
    "        \n",
    "        for i in range(hidden_layers + 1):\n",
    "            layer_len = math.ceil((i*slope + y_intercept)) if (i < hidden_layers) else output_length\n",
    "            print(\"\\tBuilding layer\", i, \"with\", layer_len, \"neurons.\")\n",
    "            new_layer = [Neuron(num_weights) for _ in range(layer_len)]\n",
    "            self.NeuronLayers.append(new_layer)\n",
    "            num_weights = layer_len\n",
    "        if (debug):\n",
    "            print(\"init sanity check:\")\n",
    "            for i in range(len(self.NeuronLayers)):\n",
    "                print(\"\\tLayer \", i,\n",
    "                      \"Width:\", len(self.NeuronLayers[i]),\n",
    "                      \"Weights:\", len(self.NeuronLayers[i][0].Weights))\n",
    "        print(\"Completed!\")\n",
    "    \n",
    "    def ForwardPropagation(self, current_input):\n",
    "        # setup previous layers \"output\" as the total input\n",
    "        layer_output = copy.deepcopy(current_input)\n",
    "        i = 0\n",
    "        for layer in self.NeuronLayers:\n",
    "            if (debug):\n",
    "                print(\"FP through layer: \", i)\n",
    "                i += 1\n",
    "            next_layer_input = []\n",
    "            for neuron in layer:\n",
    "                next_layer_input.append(neuron.Activate(layer_output))\n",
    "            layer_output = next_layer_input\n",
    "        return layer_output\n",
    "    \n",
    "    def BackwardPropagation(self, expected_value):\n",
    "        prev_layer = []\n",
    "        for layer in reversed(self.NeuronLayers):\n",
    "            layer_error = []\n",
    "            if (debug):\n",
    "                print(\"\\t\\tLengths: \", len(expected_value), \",\", len(layer))\n",
    "            if (layer is self.NeuronLayers[-1]):\n",
    "                for i in range(len(layer)):\n",
    "                    layer_error.append(expected_value[i] - layer[i].Output)\n",
    "            else:\n",
    "                for i in range(len(layer)):\n",
    "                    error = 0.0\n",
    "                    for neuron in prev_layer:\n",
    "                        error += neuron.Weights[i] * neuron.Delta\n",
    "                    layer_error.append(error)\n",
    "            for i in range(len(layer)):\n",
    "                layer[i].UpdateDelta(layer_error[i])\n",
    "            prev_layer = layer\n",
    "\n",
    "    def UpdateAllWeights(self, current_input):\n",
    "        current_input = copy.deepcopy(current_input)\n",
    "        layer_input = current_input\n",
    "        if (debug):\n",
    "            print(\"updateallweights sanity:\", layer_input)\n",
    "            i = 0\n",
    "        for layer in self.NeuronLayers:\n",
    "            if (debug):\n",
    "                print(\"\\tUpdate layer: \", i)\n",
    "                i += 1\n",
    "            next_layer_input = []\n",
    "            for neuron in layer:\n",
    "                neuron.UpdateWeights(layer_input, self.LearningRate)\n",
    "                next_layer_input.append(neuron.Output)\n",
    "            layer_input = next_layer_input\n",
    "\n",
    "    def Train(self, training_data, training_labels, epochs):\n",
    "        print(\"DNN starting training with\", epochs, \"epochs on\",\n",
    "            len(training_data),\"pieces of data.\")\n",
    "        prev_inputs = []\n",
    "        expected_similarity = 0.85\n",
    "        blank = [-1 for _ in range(self.OutputLength)]\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            print(\"Epoch:\", epoch + 1)\n",
    "            total_epoch_error = 0.0\n",
    "            \n",
    "            for i in range(len(training_data)):\n",
    "                row = training_data[i]\n",
    "                new_error = 0\n",
    "                expected_value = copy.deepcopy(blank)\n",
    "                expected_value[training_labels[i]] = 1\n",
    "                \n",
    "                if (debug):\n",
    "                    print(\"Class\", value[-1])\n",
    "                output = self.ForwardPropagation(row)\n",
    "                \n",
    "                for i in range(len(expected_value)):\n",
    "                    new_error = (expected_value[i] - output[i]) ** 2\n",
    "                if (info_dump):\n",
    "                    print(\"\\tOutput: \", output, \"\\tExpected:\", expected_value, \"\\tError: \", new_error)\n",
    "                total_epoch_error += new_error\n",
    "                self.BackwardPropagation(expected_value)\n",
    "                self.UpdateAllWeights(expected_value)\n",
    "                \n",
    "            print(\"Total Epoch Error:\", total_epoch_error, \"\\n---------------------------------\")\n",
    "\n",
    "    def Classify(self, data_row):\n",
    "        output = self.ForwardPropagation(data_row)\n",
    "        max_val = -2\n",
    "        outclass = 0\n",
    "        for i in range(len(output)):\n",
    "            if (output[i] > max_val):\n",
    "                max_val = output[i]\n",
    "                outclass = i\n",
    "        return outclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building neural network with 15 features, and 10 hidden layers.\n",
      "\tBuilding layer 0 with 15 neurons.\n",
      "\tBuilding layer 1 with 15 neurons.\n",
      "\tBuilding layer 2 with 14 neurons.\n",
      "\tBuilding layer 3 with 14 neurons.\n",
      "\tBuilding layer 4 with 13 neurons.\n",
      "\tBuilding layer 5 with 13 neurons.\n",
      "\tBuilding layer 6 with 12 neurons.\n",
      "\tBuilding layer 7 with 12 neurons.\n",
      "\tBuilding layer 8 with 11 neurons.\n",
      "\tBuilding layer 9 with 11 neurons.\n",
      "\tBuilding layer 10 with 10 neurons.\n",
      "Completed!\n",
      "DNN starting training with 200 epochs on 434 pieces of data.\n",
      "Epoch: 1\n",
      "Total Epoch Error: 511.353417479992 \n",
      "---------------------------------\n",
      "Epoch: 2\n",
      "Total Epoch Error: 469.2691207903618 \n",
      "---------------------------------\n",
      "Epoch: 3\n",
      "Total Epoch Error: 367.3191285933029 \n",
      "---------------------------------\n",
      "Epoch: 4\n",
      "Total Epoch Error: 366.8537320829791 \n",
      "---------------------------------\n",
      "Epoch: 5\n",
      "Total Epoch Error: 366.804550342176 \n",
      "---------------------------------\n",
      "Epoch: 6\n",
      "Total Epoch Error: 366.7659396547986 \n",
      "---------------------------------\n",
      "Epoch: 7\n",
      "Total Epoch Error: 366.738003613977 \n",
      "---------------------------------\n",
      "Epoch: 8\n",
      "Total Epoch Error: 366.7191791327814 \n",
      "---------------------------------\n",
      "Epoch: 9\n",
      "Total Epoch Error: 366.7063708162097 \n",
      "---------------------------------\n",
      "Epoch: 10\n",
      "Total Epoch Error: 366.6971734423891 \n",
      "---------------------------------\n",
      "Epoch: 11\n",
      "Total Epoch Error: 366.6902166386226 \n",
      "---------------------------------\n",
      "Epoch: 12\n",
      "Total Epoch Error: 366.684862341117 \n",
      "---------------------------------\n",
      "Epoch: 13\n",
      "Total Epoch Error: 366.68086541385674 \n",
      "---------------------------------\n",
      "Epoch: 14\n",
      "Total Epoch Error: 366.6781451184237 \n",
      "---------------------------------\n",
      "Epoch: 15\n",
      "Total Epoch Error: 366.6766599466532 \n",
      "---------------------------------\n",
      "Epoch: 16\n",
      "Total Epoch Error: 366.67635052664764 \n",
      "---------------------------------\n",
      "Epoch: 17\n",
      "Total Epoch Error: 366.6771216993219 \n",
      "---------------------------------\n",
      "Epoch: 18\n",
      "Total Epoch Error: 366.6788450317556 \n",
      "---------------------------------\n",
      "Epoch: 19\n",
      "Total Epoch Error: 366.68137044780144 \n",
      "---------------------------------\n",
      "Epoch: 20\n",
      "Total Epoch Error: 366.68454030017807 \n",
      "---------------------------------\n",
      "Epoch: 21\n",
      "Total Epoch Error: 366.6882020924845 \n",
      "---------------------------------\n",
      "Epoch: 22\n",
      "Total Epoch Error: 366.6922179548792 \n",
      "---------------------------------\n",
      "Epoch: 23\n",
      "Total Epoch Error: 366.6964702889478 \n",
      "---------------------------------\n",
      "Epoch: 24\n",
      "Total Epoch Error: 366.70086388061094 \n",
      "---------------------------------\n",
      "Epoch: 25\n",
      "Total Epoch Error: 366.70532528235896 \n",
      "---------------------------------\n",
      "Epoch: 26\n",
      "Total Epoch Error: 366.7098004349092 \n",
      "---------------------------------\n",
      "Epoch: 27\n",
      "Total Epoch Error: 366.71425142125116 \n",
      "---------------------------------\n",
      "Epoch: 28\n",
      "Total Epoch Error: 366.71865303540443 \n",
      "---------------------------------\n",
      "Epoch: 29\n",
      "Total Epoch Error: 366.72298960453907 \n",
      "---------------------------------\n",
      "Epoch: 30\n",
      "Total Epoch Error: 366.7272522909908 \n",
      "---------------------------------\n",
      "Epoch: 31\n",
      "Total Epoch Error: 366.73143694661127 \n",
      "---------------------------------\n",
      "Epoch: 32\n",
      "Total Epoch Error: 366.7355424959868 \n",
      "---------------------------------\n",
      "Epoch: 33\n",
      "Total Epoch Error: 366.7395697750771 \n",
      "---------------------------------\n",
      "Epoch: 34\n",
      "Total Epoch Error: 366.74352073331653 \n",
      "---------------------------------\n",
      "Epoch: 35\n",
      "Total Epoch Error: 366.7473979077017 \n",
      "---------------------------------\n",
      "Epoch: 36\n",
      "Total Epoch Error: 366.7512040882013 \n",
      "---------------------------------\n",
      "Epoch: 37\n",
      "Total Epoch Error: 366.75494210912666 \n",
      "---------------------------------\n",
      "Epoch: 38\n",
      "Total Epoch Error: 366.75861471738654 \n",
      "---------------------------------\n",
      "Epoch: 39\n",
      "Total Epoch Error: 366.7622244837097 \n",
      "---------------------------------\n",
      "Epoch: 40\n",
      "Total Epoch Error: 366.76577373576083 \n",
      "---------------------------------\n",
      "Epoch: 41\n"
     ]
    }
   ],
   "source": [
    "# features, hidden_layers, output_length, learning_rate\n",
    "dnn = DenseNeuralNetwork(len(train_features[0]) - 1, 10, 10, 0.5)\n",
    "dnn.Train(tf_train_features, tf_train_labels, 200)\n",
    "\n",
    "total_pos = 0\n",
    "print(\"Start Testing on\", len(test_features), \"test values\") \n",
    "i = 0\n",
    "for j in range(len(tf_test_features)):\n",
    "    print(\".\", end=\"\")\n",
    "    i += 1\n",
    "    if i == 100:\n",
    "        print(\"\")\n",
    "        i = 0\n",
    "    row = tf_test_features[j]\n",
    "    expected = tf_test_labels[j]\n",
    "    \n",
    "\n",
    "    actual = dnn.Classify(row[:-1])\n",
    "    total_pos += 1 if (actual == expected) else 0\n",
    "\n",
    "print(\"\\nAccuracy: \", 100 * total_pos / len(test_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num 434\n",
      "Epoch 1/200\n",
      "14/14 [==============================] - 1s 2ms/step - loss: 2.1028 - accuracy: 0.4701\n",
      "Epoch 2/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 1.8300 - accuracy: 0.6117\n",
      "Epoch 3/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 1.5775 - accuracy: 0.6633\n",
      "Epoch 4/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 1.4415 - accuracy: 0.6180\n",
      "Epoch 5/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 1.2933 - accuracy: 0.6275\n",
      "Epoch 6/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 1.0923 - accuracy: 0.7063\n",
      "Epoch 7/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 1.0503 - accuracy: 0.7323\n",
      "Epoch 8/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.9840 - accuracy: 0.7798\n",
      "Epoch 9/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.8950 - accuracy: 0.8373\n",
      "Epoch 10/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.8674 - accuracy: 0.8349\n",
      "Epoch 11/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.7493 - accuracy: 0.8679\n",
      "Epoch 12/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.7930 - accuracy: 0.8622\n",
      "Epoch 13/200\n",
      "14/14 [==============================] - 0s 1ms/step - loss: 0.7271 - accuracy: 0.8705\n",
      "Epoch 14/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6339 - accuracy: 0.8828\n",
      "Epoch 15/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6387 - accuracy: 0.8748\n",
      "Epoch 16/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6457 - accuracy: 0.8745\n",
      "Epoch 17/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5961 - accuracy: 0.8781\n",
      "Epoch 18/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5948 - accuracy: 0.8763\n",
      "Epoch 19/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5716 - accuracy: 0.8842\n",
      "Epoch 20/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6518 - accuracy: 0.8543\n",
      "Epoch 21/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5646 - accuracy: 0.8794\n",
      "Epoch 22/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.6605 - accuracy: 0.8452\n",
      "Epoch 23/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5783 - accuracy: 0.8702\n",
      "Epoch 24/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5916 - accuracy: 0.8604\n",
      "Epoch 25/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5592 - accuracy: 0.8724\n",
      "Epoch 26/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5522 - accuracy: 0.8785\n",
      "Epoch 27/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5725 - accuracy: 0.8636\n",
      "Epoch 28/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5038 - accuracy: 0.8817\n",
      "Epoch 29/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5125 - accuracy: 0.8762\n",
      "Epoch 30/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5731 - accuracy: 0.8531\n",
      "Epoch 31/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5637 - accuracy: 0.8514\n",
      "Epoch 32/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.5163 - accuracy: 0.8707\n",
      "Epoch 33/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4737 - accuracy: 0.8858\n",
      "Epoch 34/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4675 - accuracy: 0.8813\n",
      "Epoch 35/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4546 - accuracy: 0.8913\n",
      "Epoch 36/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3945 - accuracy: 0.9169\n",
      "Epoch 37/200\n",
      "14/14 [==============================] - 0s 1ms/step - loss: 0.4218 - accuracy: 0.9042\n",
      "Epoch 38/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4274 - accuracy: 0.8986\n",
      "Epoch 39/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4304 - accuracy: 0.9022\n",
      "Epoch 40/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4693 - accuracy: 0.8876\n",
      "Epoch 41/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4280 - accuracy: 0.8900\n",
      "Epoch 42/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4226 - accuracy: 0.8853\n",
      "Epoch 43/200\n",
      "14/14 [==============================] - 0s 1ms/step - loss: 0.4014 - accuracy: 0.8872\n",
      "Epoch 44/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4274 - accuracy: 0.8910\n",
      "Epoch 45/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4067 - accuracy: 0.8989\n",
      "Epoch 46/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4282 - accuracy: 0.8851\n",
      "Epoch 47/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4231 - accuracy: 0.8967\n",
      "Epoch 48/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4525 - accuracy: 0.8891\n",
      "Epoch 49/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3079 - accuracy: 0.9282\n",
      "Epoch 50/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4178 - accuracy: 0.8956\n",
      "Epoch 51/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4024 - accuracy: 0.9027\n",
      "Epoch 52/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4106 - accuracy: 0.8997\n",
      "Epoch 53/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4528 - accuracy: 0.8878\n",
      "Epoch 54/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3964 - accuracy: 0.9020\n",
      "Epoch 55/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3633 - accuracy: 0.9101\n",
      "Epoch 56/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3663 - accuracy: 0.9135\n",
      "Epoch 57/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4479 - accuracy: 0.8891\n",
      "Epoch 58/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3340 - accuracy: 0.9261\n",
      "Epoch 59/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3399 - accuracy: 0.9040\n",
      "Epoch 60/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4385 - accuracy: 0.8948\n",
      "Epoch 61/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3549 - accuracy: 0.9124\n",
      "Epoch 62/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.4251 - accuracy: 0.8834\n",
      "Epoch 63/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3188 - accuracy: 0.9220\n",
      "Epoch 64/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3806 - accuracy: 0.9034\n",
      "Epoch 65/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3666 - accuracy: 0.9108\n",
      "Epoch 66/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3274 - accuracy: 0.9211\n",
      "Epoch 67/200\n",
      "14/14 [==============================] - 0s 1ms/step - loss: 0.3696 - accuracy: 0.9009\n",
      "Epoch 68/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3631 - accuracy: 0.9111\n",
      "Epoch 69/200\n",
      "14/14 [==============================] - 0s 1ms/step - loss: 0.3589 - accuracy: 0.9102\n",
      "Epoch 70/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3315 - accuracy: 0.9228\n",
      "Epoch 71/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3783 - accuracy: 0.9005\n",
      "Epoch 72/200\n",
      "14/14 [==============================] - 0s 1ms/step - loss: 0.3284 - accuracy: 0.9210\n",
      "Epoch 73/200\n",
      "14/14 [==============================] - 0s 1ms/step - loss: 0.2876 - accuracy: 0.9356\n",
      "Epoch 74/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3015 - accuracy: 0.9245\n",
      "Epoch 75/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3713 - accuracy: 0.9061\n",
      "Epoch 76/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3565 - accuracy: 0.9128\n",
      "Epoch 77/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3557 - accuracy: 0.9099\n",
      "Epoch 78/200\n",
      "14/14 [==============================] - 0s 1ms/step - loss: 0.2920 - accuracy: 0.9276\n",
      "Epoch 79/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3542 - accuracy: 0.9075\n",
      "Epoch 80/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3287 - accuracy: 0.9230\n",
      "Epoch 81/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3595 - accuracy: 0.9002\n",
      "Epoch 82/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3450 - accuracy: 0.9153\n",
      "Epoch 83/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3705 - accuracy: 0.9036\n",
      "Epoch 84/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3546 - accuracy: 0.9083\n",
      "Epoch 85/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3476 - accuracy: 0.9087\n",
      "Epoch 86/200\n",
      "14/14 [==============================] - 0s 1ms/step - loss: 0.3671 - accuracy: 0.9070\n",
      "Epoch 87/200\n",
      "14/14 [==============================] - 0s 1ms/step - loss: 0.2957 - accuracy: 0.9235\n",
      "Epoch 88/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2980 - accuracy: 0.9233\n",
      "Epoch 89/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3121 - accuracy: 0.9201\n",
      "Epoch 90/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2853 - accuracy: 0.9282\n",
      "Epoch 91/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3476 - accuracy: 0.9126\n",
      "Epoch 92/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3136 - accuracy: 0.9116\n",
      "Epoch 93/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3368 - accuracy: 0.9112\n",
      "Epoch 94/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3168 - accuracy: 0.9163\n",
      "Epoch 95/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3110 - accuracy: 0.9159\n",
      "Epoch 96/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3446 - accuracy: 0.8994\n",
      "Epoch 97/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3217 - accuracy: 0.9160\n",
      "Epoch 98/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2867 - accuracy: 0.9232\n",
      "Epoch 99/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3056 - accuracy: 0.9036\n",
      "Epoch 100/200\n",
      "14/14 [==============================] - 0s 1ms/step - loss: 0.2522 - accuracy: 0.9420\n",
      "Epoch 101/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2935 - accuracy: 0.9176\n",
      "Epoch 102/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3652 - accuracy: 0.9096\n",
      "Epoch 103/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2740 - accuracy: 0.9216\n",
      "Epoch 104/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3298 - accuracy: 0.9079\n",
      "Epoch 105/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2693 - accuracy: 0.9378\n",
      "Epoch 106/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3259 - accuracy: 0.9153\n",
      "Epoch 107/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3402 - accuracy: 0.9019\n",
      "Epoch 108/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3163 - accuracy: 0.9272\n",
      "Epoch 109/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3392 - accuracy: 0.9079\n",
      "Epoch 110/200\n",
      "14/14 [==============================] - 0s 1ms/step - loss: 0.2928 - accuracy: 0.9349\n",
      "Epoch 111/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3004 - accuracy: 0.9179\n",
      "Epoch 112/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2987 - accuracy: 0.9164\n",
      "Epoch 113/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3352 - accuracy: 0.9166\n",
      "Epoch 114/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2725 - accuracy: 0.9275\n",
      "Epoch 115/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2944 - accuracy: 0.9296\n",
      "Epoch 116/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3976 - accuracy: 0.8956\n",
      "Epoch 117/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3296 - accuracy: 0.9149\n",
      "Epoch 118/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3322 - accuracy: 0.9212\n",
      "Epoch 119/200\n",
      "14/14 [==============================] - 0s 1ms/step - loss: 0.2912 - accuracy: 0.9212\n",
      "Epoch 120/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3081 - accuracy: 0.9184\n",
      "Epoch 121/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2717 - accuracy: 0.9265\n",
      "Epoch 122/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2483 - accuracy: 0.9448\n",
      "Epoch 123/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3153 - accuracy: 0.9121\n",
      "Epoch 124/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2294 - accuracy: 0.9508\n",
      "Epoch 125/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2992 - accuracy: 0.9144\n",
      "Epoch 126/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3140 - accuracy: 0.9210\n",
      "Epoch 127/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2993 - accuracy: 0.9168\n",
      "Epoch 128/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2455 - accuracy: 0.9382\n",
      "Epoch 129/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3525 - accuracy: 0.9137\n",
      "Epoch 130/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2557 - accuracy: 0.9376\n",
      "Epoch 131/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3389 - accuracy: 0.9134\n",
      "Epoch 132/200\n",
      "14/14 [==============================] - 0s 1ms/step - loss: 0.2380 - accuracy: 0.9450\n",
      "Epoch 133/200\n",
      "14/14 [==============================] - 0s 1ms/step - loss: 0.2371 - accuracy: 0.9388\n",
      "Epoch 134/200\n",
      "14/14 [==============================] - 0s 1ms/step - loss: 0.2772 - accuracy: 0.9285\n",
      "Epoch 135/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2600 - accuracy: 0.9368\n",
      "Epoch 136/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2552 - accuracy: 0.9328\n",
      "Epoch 137/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2810 - accuracy: 0.9233\n",
      "Epoch 138/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2738 - accuracy: 0.9169\n",
      "Epoch 139/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2229 - accuracy: 0.9412\n",
      "Epoch 140/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2464 - accuracy: 0.9327\n",
      "Epoch 141/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2902 - accuracy: 0.9253\n",
      "Epoch 142/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2641 - accuracy: 0.9252\n",
      "Epoch 143/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2700 - accuracy: 0.9257\n",
      "Epoch 144/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2912 - accuracy: 0.9261\n",
      "Epoch 145/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2603 - accuracy: 0.9292\n",
      "Epoch 146/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2710 - accuracy: 0.9250\n",
      "Epoch 147/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2352 - accuracy: 0.9414\n",
      "Epoch 148/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2682 - accuracy: 0.9328\n",
      "Epoch 149/200\n",
      "14/14 [==============================] - 0s 1ms/step - loss: 0.2652 - accuracy: 0.9288\n",
      "Epoch 150/200\n",
      "14/14 [==============================] - 0s 1ms/step - loss: 0.2955 - accuracy: 0.9136\n",
      "Epoch 151/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2731 - accuracy: 0.9246\n",
      "Epoch 152/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2657 - accuracy: 0.9268\n",
      "Epoch 153/200\n",
      "14/14 [==============================] - 0s 1ms/step - loss: 0.2247 - accuracy: 0.9448\n",
      "Epoch 154/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2577 - accuracy: 0.9252\n",
      "Epoch 155/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2896 - accuracy: 0.9281\n",
      "Epoch 156/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2867 - accuracy: 0.9172\n",
      "Epoch 157/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2292 - accuracy: 0.9407\n",
      "Epoch 158/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2585 - accuracy: 0.9345\n",
      "Epoch 159/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2481 - accuracy: 0.9299\n",
      "Epoch 160/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2527 - accuracy: 0.9302\n",
      "Epoch 161/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2439 - accuracy: 0.9388\n",
      "Epoch 162/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2500 - accuracy: 0.9349\n",
      "Epoch 163/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2759 - accuracy: 0.9235\n",
      "Epoch 164/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2441 - accuracy: 0.9248\n",
      "Epoch 165/200\n",
      "14/14 [==============================] - 0s 1ms/step - loss: 0.3077 - accuracy: 0.9249\n",
      "Epoch 166/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2947 - accuracy: 0.9201\n",
      "Epoch 167/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2421 - accuracy: 0.9402\n",
      "Epoch 168/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2770 - accuracy: 0.9202\n",
      "Epoch 169/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3022 - accuracy: 0.9180\n",
      "Epoch 170/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2450 - accuracy: 0.9413\n",
      "Epoch 171/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2286 - accuracy: 0.9450\n",
      "Epoch 172/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2141 - accuracy: 0.9535\n",
      "Epoch 173/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2117 - accuracy: 0.9434\n",
      "Epoch 174/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2649 - accuracy: 0.9346\n",
      "Epoch 175/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2578 - accuracy: 0.9245\n",
      "Epoch 176/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2201 - accuracy: 0.9465\n",
      "Epoch 177/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2171 - accuracy: 0.9418\n",
      "Epoch 178/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2482 - accuracy: 0.9376\n",
      "Epoch 179/200\n",
      "14/14 [==============================] - 0s 1ms/step - loss: 0.2477 - accuracy: 0.9445\n",
      "Epoch 180/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2188 - accuracy: 0.9405\n",
      "Epoch 181/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2564 - accuracy: 0.9334\n",
      "Epoch 182/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2882 - accuracy: 0.9255\n",
      "Epoch 183/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2026 - accuracy: 0.9407\n",
      "Epoch 184/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2851 - accuracy: 0.9281\n",
      "Epoch 185/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2298 - accuracy: 0.9334\n",
      "Epoch 186/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2259 - accuracy: 0.9489\n",
      "Epoch 187/200\n",
      "14/14 [==============================] - 0s 1ms/step - loss: 0.2471 - accuracy: 0.9413\n",
      "Epoch 188/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.1997 - accuracy: 0.9572\n",
      "Epoch 189/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2233 - accuracy: 0.9415\n",
      "Epoch 190/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2029 - accuracy: 0.9498\n",
      "Epoch 191/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2650 - accuracy: 0.9240\n",
      "Epoch 192/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2313 - accuracy: 0.9454\n",
      "Epoch 193/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2273 - accuracy: 0.9416\n",
      "Epoch 194/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2152 - accuracy: 0.9452\n",
      "Epoch 195/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2455 - accuracy: 0.9324\n",
      "Epoch 196/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2280 - accuracy: 0.9420\n",
      "Epoch 197/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2718 - accuracy: 0.9206\n",
      "Epoch 198/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2480 - accuracy: 0.9305\n",
      "Epoch 199/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2040 - accuracy: 0.9464\n",
      "Epoch 200/200\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.2578 - accuracy: 0.9259\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.5832 - accuracy: 0.8618\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_6 (Flatten)          (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 14)                224       \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 13)                195       \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 12)                168       \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 11)                143       \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 10)                120       \n",
      "=================================================================\n",
      "Total params: 850\n",
      "Trainable params: 850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epochs: 200\n",
      "<tensorflow.python.keras.callbacks.History object at 0x000001B7DC818250>\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# change data to fit.\n",
    "def eval_model(dataset, model, epochs=10):\n",
    "    train_images, train_labels, test_images, test_labels = dataset\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    print ('num', len(train_labels))\n",
    "    hist = model.fit(train_images, train_labels, epochs=epochs)\n",
    "    #train_acc = hist.history['acc'][-1]\n",
    "    test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "    model.summary()\n",
    "    print('Epochs: ' + str(epochs))\n",
    "    print(hist)\n",
    "    #print('Training accuracy: ' + str(train_acc))\n",
    "    #print('Testing accuracy: ' + str(test_acc))\n",
    "\n",
    "def run_model(epochs=200):\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Flatten(input_shape=(15, 1)),\n",
    "        keras.layers.Dense(units=14, activation=tf.nn.tanh),\n",
    "        keras.layers.Dense(units=13, activation=tf.nn.tanh),\n",
    "        keras.layers.Dense(units=12, activation=tf.nn.tanh),\n",
    "        keras.layers.Dense(units=11, activation=tf.nn.tanh),\n",
    "        keras.layers.Dense(units=10, activation=tf.nn.softmax)\n",
    "        ])\n",
    "    eval_model(tf_dataset, model, epochs)\n",
    "\n",
    "run_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:  [[ 1  0  0  0  0  1  0  0  0]\n",
      " [ 0  8  0  0  0  1  2  0  0]\n",
      " [ 0  0  8  1  0  0  0  0  0]\n",
      " [ 0  0  0 14  0  1  0  1  0]\n",
      " [ 0  0  0  0  1  0  0  0  0]\n",
      " [ 2  1  0  2  0 37  0  0  0]\n",
      " [ 0  0  0  0  0  3 22  0  0]\n",
      " [ 0  0  0  0  0  1  0  1  0]\n",
      " [ 0  0  3  0  0  0  0  0  0]]\n",
      "Accuracy score:  82.88288288288288\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.50      0.40         2\n",
      "           1       0.89      0.73      0.80        11\n",
      "           2       0.73      0.89      0.80         9\n",
      "           3       0.82      0.88      0.85        16\n",
      "           4       1.00      1.00      1.00         1\n",
      "           5       0.84      0.88      0.86        42\n",
      "           6       0.92      0.88      0.90        25\n",
      "           7       0.50      0.50      0.50         2\n",
      "           9       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.83       111\n",
      "   macro avg       0.67      0.69      0.68       111\n",
      "weighted avg       0.81      0.83      0.82       111\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\elisa\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of feature_names, 19 does not match number of features, 15",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-66cbaac5fc58>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot_tree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m dot_data = tree.export_graphviz(clf, out_file=None, \n\u001b[0m\u001b[0;32m     24\u001b[0m                       \u001b[0mfeature_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrow_swaps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m                       \u001b[0mclass_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mArtists\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_export.py\u001b[0m in \u001b[0;36mexport_graphviz\u001b[1;34m(decision_tree, out_file, max_depth, feature_names, class_names, label, filled, leaves_parallel, impurity, node_ids, proportion, rotate, rounded, special_characters, precision)\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[0mrounded\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrounded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspecial_characters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mspecial_characters\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m             precision=precision)\n\u001b[1;32m--> 783\u001b[1;33m         \u001b[0mexporter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecision_tree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    784\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mreturn_string\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_export.py\u001b[0m in \u001b[0;36mexport\u001b[1;34m(self, decision_tree)\u001b[0m\n\u001b[0;32m    411\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_names\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    412\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mdecision_tree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_features_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 413\u001b[1;33m                 raise ValueError(\"Length of feature_names, %d \"\n\u001b[0m\u001b[0;32m    414\u001b[0m                                  \u001b[1;34m\"does not match number of features, %d\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    415\u001b[0m                                  % (len(self.feature_names),\n",
      "\u001b[1;31mValueError\u001b[0m: Length of feature_names, 19 does not match number of features, 15"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_train = tf_dataset[0]\n",
    "Y_train = tf_dataset[1]\n",
    "X_test = tf_dataset[2]\n",
    "Y_test = tf_dataset[3]\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X_train, Y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Confusion Matrix: \", confusion_matrix(Y_test, y_pred))\n",
    "acc = accuracy_score(Y_test, y_pred)*100\n",
    "print(\"Accuracy score: \", acc)\n",
    "print(\"Classification Report: \\n\", classification_report(Y_test, y_pred))\n",
    "\n",
    "tree.plot_tree(clf)\n",
    "dot_data = tree.export_graphviz(clf, out_file=None, \n",
    "                      feature_names=row_swaps,  \n",
    "                      class_names=Artists,  \n",
    "                      filled=True, rounded=True,  \n",
    "                      special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
